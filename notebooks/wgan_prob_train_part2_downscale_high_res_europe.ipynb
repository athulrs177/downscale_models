{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69462c4e-76ba-47a6-a78f-4ff8b63e5aea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:16:17.105505: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-29 14:16:17.105557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-29 14:16:17.106885: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-29 14:16:17.125638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 14:16:19.290750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Conv2DTranspose, MaxPool2D, Flatten, Conv2D, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization, UpSampling2D, concatenate, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Input\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Dropout, Concatenate, LeakyReLU, BatchNormalization, Add\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm \n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MaxAbsScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "import properscoring as ps\n",
    "\n",
    "tf.random.set_seed(42)  # to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a063a2-6f1c-4924-8242-425a88cfb819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137f9c7f-5d41-4a53-9db0-a4a9aa56a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:16:27.097474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 75570 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2024-08-29 14:16:27.099084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 75570 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:44:00.0, compute capability: 8.0\n",
      "2024-08-29 14:16:27.100545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 75570 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:84:00.0, compute capability: 8.0\n",
      "2024-08-29 14:16:27.101936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 75570 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:c4:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Get the list of available physical GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Set memory growth for each GPU\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Create MirroredStrategy\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"])\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d876839-26e3-482c-a342-5824d9f0c4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diri = '/work/bb0983/athul_satheesh/e_obs_precip/'\n",
    "diro = '/work/bb0983/athul_satheesh/downscaled_data/europe/figures/'\n",
    "\n",
    "coarse_raw = 'rr_ens_mean_1.0deg_reg_v29.0e.nc'\n",
    "fine_raw = 'rr_ens_mean_0.1deg_reg_v29.0e.nc'\n",
    "\n",
    "lati = 43#40\n",
    "latf = 59#60\n",
    "\n",
    "loni = -6#-10\n",
    "lonf = 15#30\n",
    "\n",
    "strt = '1950-01-01'\n",
    "last = '2023-12-31'\n",
    "\n",
    "coarse_data = xr.open_dataset(diri+coarse_raw).rr.transpose('time','lat','lon').sel(time=slice(strt, last), \n",
    "                                                                                    lat=slice(lati, latf), \n",
    "                                                                                    lon=slice(loni, lonf)\n",
    "                                                                                   )\n",
    "fine_data = xr.open_dataset(diri+fine_raw).rr.transpose('time','latitude','longitude').sel(time=slice(strt, last), \n",
    "                                                                                           latitude=slice(lati, latf), \n",
    "                                                                                           longitude=slice(loni, lonf)\n",
    "                                                                                          )\n",
    "fine_data = fine_data.rename({'latitude':'lat', 'longitude':'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5df139-59b3-4c0f-b01f-db6c717413f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_coarse = coarse_data.shape\n",
    "dims_fine = fine_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62136626-8e1a-4880-9706-1b7fbac4de4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27028, 16, 21), (27028, 160, 210))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims_coarse, dims_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e96e34b-64dd-40a1-a62c-464150bfd655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_strt = strt\n",
    "train_last = '2000-12-31'\n",
    "\n",
    "test_strt = '2001-01-01'\n",
    "test_last = last\n",
    "\n",
    "coarse_data_train = coarse_data.sel(time=slice(train_strt, train_last))#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_train = coarse_data_train.fillna(coarse_data_train.mean())\n",
    "coarse_data_test = coarse_data.sel(time=slice(test_strt, test_last))#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_test = coarse_data_test.fillna(coarse_data_test.mean())\n",
    "# print(coarse_data_train.shape, coarse_data_test.shape)\n",
    "\n",
    "fine_data_train = fine_data.sel(time=slice(train_strt, train_last))\n",
    "fine_data_test = fine_data.sel(time=slice(test_strt, test_last))\n",
    "# print(fine_data_train.shape, fine_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c02225-58ae-43f0-99e2-10aebc02f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_nan_mask = np.isnan(coarse_data_test)\n",
    "fine_nan_mask = np.isnan(fine_data_test)\n",
    "\n",
    "fill_val = -1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b631b060-a6bb-4fe6-90ac-55b42c993ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_data_train = coarse_data_train.fillna(fill_val)#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_train = coarse_data_train.fillna(coarse_data_train.mean())\n",
    "coarse_data_test = coarse_data_test.fillna(fill_val)#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_test = coarse_data_test.fillna(coarse_data_test.mean())\n",
    "# print(coarse_data_train.shape, coarse_data_test.shape)\n",
    "\n",
    "fine_data_train = fine_data_train.fillna(fill_val)\n",
    "fine_data_test = fine_data_test.fillna(fill_val)\n",
    "# print(fine_data_train.shape, fine_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa2aa60c-3df9-42ae-8a88-c36d736c1e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 160, 210)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df65693-9401-4fb3-b807-624a93043b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_loss(fake_images, real_images):\n",
    "    \"\"\"\n",
    "    Computes the SSIM loss between fake and real images.\n",
    "    \n",
    "    Parameters:\n",
    "        fake_images (tf.Tensor): Generated images.\n",
    "        real_images (tf.Tensor): Real images.\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: SSIM loss.\n",
    "    \"\"\"\n",
    "    ssim_index = tf.image.ssim(fake_images, real_images, max_val=1.0)\n",
    "    return 1 - tf.reduce_mean(ssim_index)\n",
    "\n",
    "def generator_loss(fake_output, fake_images, real_images, penalty_weight=15, ssim_weight=15):\n",
    "    \"\"\"\n",
    "    Generator loss function using Wasserstein loss with an added penalty term and SSIM loss.\n",
    "    \n",
    "    Parameters:\n",
    "        fake_output (tf.Tensor): Output of the discriminator when given generated images.\n",
    "        fake_images (tf.Tensor): Generated images.\n",
    "        real_images (tf.Tensor): Real images.\n",
    "        penalty_weight (float): Weight of the penalty term.\n",
    "        ssim_weight (float): Weight of the SSIM loss term.\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: Generator loss.\n",
    "    \"\"\"\n",
    "    wasserstein_loss = -tf.reduce_mean(fake_output)\n",
    "    \n",
    "    # Penalty term for deviation from real outputs\n",
    "    # penalty = penalty_weight * tf.reduce_mean(tf.abs(fake_output - real_output))\n",
    "    penalty = penalty_weight * tf.reduce_mean(tf.abs(fake_images - real_images))\n",
    "    \n",
    "    # SSIM loss\n",
    "    ssim_loss_value = ssim_loss(fake_images, real_images)\n",
    "    \n",
    "    return wasserstein_loss + penalty + ssim_weight * ssim_loss_value\n",
    "    # return wasserstein_loss + ssim_weight * ssim_loss_value\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    Discriminator loss function using Wasserstein loss.\n",
    "    \n",
    "    Parameters:\n",
    "        real_output (tf.Tensor): Output of the discriminator when given real images.\n",
    "        fake_output (tf.Tensor): Output of the discriminator when given generated images.\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: Discriminator loss.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dfff4f-9339-4189-93bc-156b79d5514d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weight clipping\n",
    "def clip_weights(model, clip_value):\n",
    "    \"\"\"\n",
    "    Clips the weights of the model to be within the range [-clip_value, clip_value].\n",
    "    \n",
    "    Parameters:\n",
    "        model (tf.keras.Model): The model whose weights will be clipped.\n",
    "        clip_value (float): The value to clip the weights to.\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            kernel = layer.kernel\n",
    "            clipped_kernel = tf.clip_by_value(kernel, -clip_value, clip_value)\n",
    "            layer.kernel.assign(clipped_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7deb6976-a4fe-422a-ba87-939ed5daf0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a custom dropout layer that is active also during prediction and use this in the model instead of the default Dropout.\n",
    "# Alternatively you can also set \"training=True\" in the Dropout layer\n",
    "@register_keras_serializable()\n",
    "class CustomDropout(Dropout):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(rate, **kwargs)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return super().call(inputs, training=True)  # Always active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fbc1a4-4a45-45b6-82b3-1b82a1caf742",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_in = 1550 #1450 #600 #850 #450\n",
    "\n",
    "model_diri = '/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/' #'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp/'\n",
    "gen_model_fili = f'generator_prob_epoch_{epoch_in}_adamV5.keras'\n",
    "dis_model_fili = f'discriminator_prob_epoch_{epoch_in}_adamV5.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ffd9be-da59-4c7c-809b-f1f64ffa003a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "input_shape  = dims_coarse[1:] + (1,)\n",
    "output_shape = dims_fine[1:] + (1,)\n",
    "\n",
    "gen_lr = 1.0e-5 # modified from 2e-5 after 1550 epochs; modified from 2e-4 after 1000 epochs\n",
    "dis_lr = 7.5e-7 # modified from 1e-6 after 1550 epochs; modified from 1e-5 after 1000 epochs\n",
    "\n",
    "# gen_lr = 1e-4 # changed from 2e-4 after 150 epochs; kept the same for next another 900 epochs; changed to 1e-4 after 1450 epochs\n",
    "# dis_lr = 1e-6 # changed from 2e-4 after 150 epochs; changed to 0.5e-5 after 1050 epochs; changed to 1e-6 after 1450 epochs \n",
    "clip_value = 1e-3 \n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    # gen_optimizer = RMSprop(learning_rate=gen_lr, momentum=0.5)\n",
    "    # dis_optimizer = RMSprop(learning_rate=dis_lr, momentum=0.5)\n",
    "    gen_optimizer = Adam(learning_rate=gen_lr)\n",
    "    dis_optimizer = Adam(learning_rate=dis_lr)\n",
    "    \n",
    "    gen = load_model(model_diri+gen_model_fili)\n",
    "    dis = load_model(model_diri+dis_model_fili)\n",
    "\n",
    "    gen.compile(optimizer=gen_optimizer,)\n",
    "    dis.compile(optimizer=dis_optimizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4838ac4-2d1f-46a0-9bf3-acc9bcee8ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " generator_input (InputLaye  [(None, 16, 21, 1)]       0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " conv_4_conv (Conv2D)        (None, 16, 21, 16)        160       \n",
      "                                                                 \n",
      " conv_4_lrelu (LeakyReLU)    (None, 16, 21, 16)        0         \n",
      "                                                                 \n",
      " conv_4_bn (BatchNormalizat  (None, 16, 21, 16)        64        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_4_dropout (Dropout)    (None, 16, 21, 16)        0         \n",
      "                                                                 \n",
      " conv_3_conv (Conv2D)        (None, 16, 21, 32)        4640      \n",
      "                                                                 \n",
      " conv_3_lrelu (LeakyReLU)    (None, 16, 21, 32)        0         \n",
      "                                                                 \n",
      " conv_3_bn (BatchNormalizat  (None, 16, 21, 32)        128       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_3_dropout (Dropout)    (None, 16, 21, 32)        0         \n",
      "                                                                 \n",
      " conv_2_conv (Conv2D)        (None, 16, 21, 64)        18496     \n",
      "                                                                 \n",
      " conv_2_lrelu (LeakyReLU)    (None, 16, 21, 64)        0         \n",
      "                                                                 \n",
      " conv_2_bn (BatchNormalizat  (None, 16, 21, 64)        256       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_2_dropout (Dropout)    (None, 16, 21, 64)        0         \n",
      "                                                                 \n",
      " conv_1_conv (Conv2D)        (None, 16, 21, 128)       73856     \n",
      "                                                                 \n",
      " conv_1_lrelu (LeakyReLU)    (None, 16, 21, 128)       0         \n",
      "                                                                 \n",
      " conv_1_bn (BatchNormalizat  (None, 16, 21, 128)       512       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_1_dropout (Dropout)    (None, 16, 21, 128)       0         \n",
      "                                                                 \n",
      " conv_0_conv (Conv2D)        (None, 16, 21, 256)       295168    \n",
      "                                                                 \n",
      " conv_0_lrelu (LeakyReLU)    (None, 16, 21, 256)       0         \n",
      "                                                                 \n",
      " conv_0_bn (BatchNormalizat  (None, 16, 21, 256)       1024      \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_0_dropout (Dropout)    (None, 16, 21, 256)       0         \n",
      "                                                                 \n",
      " conv_00_conv (Conv2D)       (None, 16, 21, 512)       1180160   \n",
      "                                                                 \n",
      " conv_00_lrelu (LeakyReLU)   (None, 16, 21, 512)       0         \n",
      "                                                                 \n",
      " conv_00_bn (BatchNormaliza  (None, 16, 21, 512)       2048      \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv_00_dropout (Dropout)   (None, 16, 21, 512)       0         \n",
      "                                                                 \n",
      " deconv_00_deconv (Conv2DTr  (None, 16, 21, 512)       2359808   \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " deconv_00_lrelu (LeakyReLU  (None, 16, 21, 512)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " deconv_00_bn (BatchNormali  (None, 16, 21, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " deconv_00_dropout (Dropout  (None, 16, 21, 512)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " deconv_0_deconv (Conv2DTra  (None, 16, 21, 256)       1179904   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_0_lrelu (LeakyReLU)  (None, 16, 21, 256)       0         \n",
      "                                                                 \n",
      " deconv_0_bn (BatchNormaliz  (None, 16, 21, 256)       1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_0_dropout (Dropout)  (None, 16, 21, 256)       0         \n",
      "                                                                 \n",
      " deconv_1_deconv (Conv2DTra  (None, 16, 21, 128)       295040    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_1_lrelu (LeakyReLU)  (None, 16, 21, 128)       0         \n",
      "                                                                 \n",
      " deconv_1_bn (BatchNormaliz  (None, 16, 21, 128)       512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_1_dropout (Dropout)  (None, 16, 21, 128)       0         \n",
      "                                                                 \n",
      " deconv_2_deconv (Conv2DTra  (None, 16, 21, 64)        73792     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_2_lrelu (LeakyReLU)  (None, 16, 21, 64)        0         \n",
      "                                                                 \n",
      " deconv_2_bn (BatchNormaliz  (None, 16, 21, 64)        256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_2_dropout (Dropout)  (None, 16, 21, 64)        0         \n",
      "                                                                 \n",
      " deconv_3_deconv (Conv2DTra  (None, 16, 21, 32)        18464     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_3_lrelu (LeakyReLU)  (None, 16, 21, 32)        0         \n",
      "                                                                 \n",
      " deconv_3_bn (BatchNormaliz  (None, 16, 21, 32)        128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_3_dropout (Dropout)  (None, 16, 21, 32)        0         \n",
      "                                                                 \n",
      " deconv_4_deconv (Conv2DTra  (None, 80, 105, 16)       18448     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_4_lrelu (LeakyReLU)  (None, 80, 105, 16)       0         \n",
      "                                                                 \n",
      " deconv_4_bn (BatchNormaliz  (None, 80, 105, 16)       64        \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_4_dropout (Dropout)  (None, 80, 105, 16)       0         \n",
      "                                                                 \n",
      " final_deconv (Conv2DTransp  (None, 160, 210, 1)       145       \n",
      " ose)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5526145 (21.08 MB)\n",
      "Trainable params: 5522113 (21.07 MB)\n",
      "Non-trainable params: 4032 (15.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a532b24-5242-48d5-b62c-df4de0b79378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"wGAN_discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " discriminator_input (Input  [(None, 160, 210, 1)]     0         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " conv1_conv (Conv2D)         (None, 80, 105, 128)      1280      \n",
      "                                                                 \n",
      " conv1_lrelu (LeakyReLU)     (None, 80, 105, 128)      0         \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizati  (None, 80, 105, 128)      512       \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv3_conv (Conv2D)         (None, 80, 105, 256)      295168    \n",
      "                                                                 \n",
      " conv3_lrelu (LeakyReLU)     (None, 80, 105, 256)      0         \n",
      "                                                                 \n",
      " conv3_bn (BatchNormalizati  (None, 80, 105, 256)      1024      \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv5_conv (Conv2D)         (None, 80, 105, 512)      1180160   \n",
      "                                                                 \n",
      " conv5_lrelu (LeakyReLU)     (None, 80, 105, 512)      0         \n",
      "                                                                 \n",
      " conv5_bn (BatchNormalizati  (None, 80, 105, 512)      2048      \n",
      " on)                                                             \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 40, 52, 512)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " final_conv (Conv2D)         (None, 40, 52, 1)         4609      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1484801 (5.66 MB)\n",
      "Trainable params: 1483009 (5.66 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "160299f8-d49d-4cc5-90fa-c48fc83fc019",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "def preprocess_data(data):\n",
    "    data = tf.convert_to_tensor(data.values, dtype=tf.float32)\n",
    "    return tf.reshape(data, data.shape + (1,))\n",
    "# Compute and update gradients\n",
    "def train_step(coarse_data_batch, fine_data_batch, dis, clip_value):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "        fake_data_batch = gen(coarse_data_batch, training=True)\n",
    "        real_output = dis(fine_data_batch, training=True)\n",
    "        fake_output = dis(fake_data_batch, training=True)\n",
    "        gen_loss = generator_loss(fake_output, fake_data_batch, fine_data_batch)\n",
    "        dis_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "\n",
    "    gradients_of_gen = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
    "    gradients_of_dis = dis_tape.gradient(dis_loss, dis.trainable_variables)\n",
    "    \n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_gen, gen.trainable_variables))\n",
    "    dis_optimizer.apply_gradients(zip(gradients_of_dis, dis.trainable_variables))\n",
    "    \n",
    "    # Clip discriminator weights\n",
    "    clip_weights(dis, clip_value)\n",
    "    \n",
    "    return gen_loss, dis_loss\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(coarse_data_batch, fine_data_batch, dis, clip_value):\n",
    "    per_replica_gen_losses, per_replica_dis_losses = strategy.run(train_step, args=(coarse_data_batch, fine_data_batch, dis, clip_value))\n",
    "    mean_gen_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_gen_losses, axis=None)\n",
    "    mean_dis_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_dis_losses, axis=None)\n",
    "    return mean_gen_loss, mean_dis_loss\n",
    "\n",
    "def train_gan(gen, dis, coarse_data_train, fine_data_train, clip_value, epochs, batch_size, save_intermediate=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((preprocess_data(coarse_data_train), preprocess_data(fine_data_train)))\n",
    "    # dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat(epochs)\n",
    "    dataset = dataset.shuffle(buffer_size=365*2).batch(batch_size)#.repeat(epochs)\n",
    "    distributed_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_gen_loss = 0.0\n",
    "        total_dis_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for coarse_data_batch, fine_data_batch in distributed_dataset:\n",
    "            gen_loss, dis_loss = distributed_train_step(coarse_data_batch, fine_data_batch, dis, clip_value)\n",
    "            total_gen_loss += gen_loss\n",
    "            total_dis_loss += dis_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_gen_loss = total_gen_loss / num_batches\n",
    "        avg_dis_loss = total_dis_loss / num_batches\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch_in+epoch+1}, Generator Loss: {avg_gen_loss:.5f}, Discriminator Loss: {avg_dis_loss:.5f}\")\n",
    "\n",
    "        # Save models every 100 epochs\n",
    "        if save_intermediate:\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                gen.save(f'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/generator_prob_epoch_{epoch_in+epoch+1}_adamV5.keras')\n",
    "                dis.save(f'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/discriminator_prob_epoch_{epoch_in+epoch+1}_adamV5.keras')\n",
    "\n",
    "#### Modification: Train the critic 5x as much for one iteration of generator training ####\n",
    "# def train_gan(gen, dis, coarse_data_train, fine_data_train, clip_value, epochs, batch_size, n_critic=5, save_intermediate=True):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((preprocess_data(coarse_data_train), preprocess_data(fine_data_train)))\n",
    "#     dataset = dataset.shuffle(buffer_size=365*2).batch(batch_size)\n",
    "#     distributed_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         total_gen_loss = 0.0\n",
    "#         total_dis_loss = 0.0\n",
    "#         num_batches = 0\n",
    "\n",
    "#         for coarse_data_batch, fine_data_batch in distributed_dataset:\n",
    "#             # Train the discriminator for n_critic iterations\n",
    "#             for _ in range(n_critic):\n",
    "#                 _, dis_loss = distributed_train_step(coarse_data_batch, fine_data_batch, dis, clip_value)\n",
    "#                 total_dis_loss += dis_loss\n",
    "#                 num_batches += 1\n",
    "\n",
    "#             # Train the generator once\n",
    "#             gen_loss, _ = distributed_train_step(coarse_data_batch, fine_data_batch, dis, clip_value)\n",
    "#             total_gen_loss += gen_loss\n",
    "\n",
    "#         avg_gen_loss = total_gen_loss / num_batches\n",
    "#         avg_dis_loss = total_dis_loss / (num_batches * n_critic)\n",
    "\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             print(f\"Epoch {epoch + 1}, Generator Loss: {avg_gen_loss:.5f}, Discriminator Loss: {avg_dis_loss:.5f}\")\n",
    "\n",
    "#         # Save models every 100 epochs\n",
    "#         if save_intermediate:\n",
    "#             if (epoch + 1) % 50 == 0:\n",
    "#                 gen.save(f'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/generator_prob_epoch_{epoch_in+epoch+1}_adamV4.keras')\n",
    "#                 dis.save(f'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/discriminator_prob_epoch_{epoch_in+epoch+1}_adamV4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0536200-3174-49d6-9da3-ccbc65b1000a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def normalize_data(dataset, scaler, train=True):\n",
    "    \n",
    "#     dims = dataset.shape\n",
    "    \n",
    "#     dataset = dataset.values.reshape(-1, dims[1]*dims[2])\n",
    "\n",
    "#     if train:\n",
    "#         normalized_data = scaler.fit_transform(dataset)\n",
    "#     else:\n",
    "#         normalized_data = scaler.transform(dataset)\n",
    "        \n",
    "#     normalized_data = normalized_data.reshape(dims + (1,) ) # Make 4D\n",
    "    \n",
    "#     return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dd48624-396c-4587-af41-bbd009eb9d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1000 \n",
    "batch_size = 365*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "015e29b3-4acf-4a3b-87c2-9024a8313cf8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:17:24.549920: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 50 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 14 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 50 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 14 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:17:41.762524: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inreplica_2/generator/conv_4_dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-08-29 14:17:43.582759: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-29 14:17:43.586779: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-29 14:17:43.740983: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-29 14:17:43.808544: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-29 14:17:43.907559: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-08-29 14:22:31.488752: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd4b64f4ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-29 14:22:31.488804: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-29 14:22:31.488810: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-29 14:22:31.488814: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-29 14:22:31.488818: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-29 14:22:31.520078: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724934151.687563 1921747 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 50 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 14 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:22:59.696995: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inreplica_2/generator/conv_4_dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1560, Generator Loss: 56.90831, Discriminator Loss: -3.17197\n",
      "Epoch 1570, Generator Loss: 56.91593, Discriminator Loss: -3.18277\n",
      "Epoch 1580, Generator Loss: 56.84587, Discriminator Loss: -3.13732\n",
      "Epoch 1590, Generator Loss: 56.84227, Discriminator Loss: -3.11761\n",
      "Epoch 1600, Generator Loss: 56.85524, Discriminator Loss: -3.11181\n",
      "Epoch 1610, Generator Loss: 56.87497, Discriminator Loss: -3.23412\n",
      "Epoch 1620, Generator Loss: 56.89557, Discriminator Loss: -3.04199\n",
      "Epoch 1630, Generator Loss: 56.84678, Discriminator Loss: -3.15021\n",
      "Epoch 1640, Generator Loss: 58.22386, Discriminator Loss: -2.27078\n",
      "Epoch 1650, Generator Loss: 57.38395, Discriminator Loss: -2.82293\n",
      "Epoch 1660, Generator Loss: 57.19876, Discriminator Loss: -2.97130\n",
      "Epoch 1670, Generator Loss: 57.14486, Discriminator Loss: -3.13462\n",
      "Epoch 1680, Generator Loss: 57.04327, Discriminator Loss: -3.14307\n",
      "Epoch 1690, Generator Loss: 57.02158, Discriminator Loss: -3.20574\n",
      "Epoch 1700, Generator Loss: 56.94446, Discriminator Loss: -3.21225\n",
      "Epoch 1710, Generator Loss: 56.99380, Discriminator Loss: -3.19625\n",
      "Epoch 1720, Generator Loss: 56.89182, Discriminator Loss: -3.23652\n",
      "Epoch 1730, Generator Loss: 56.93114, Discriminator Loss: -3.18312\n",
      "Epoch 1740, Generator Loss: 56.94756, Discriminator Loss: -3.27597\n",
      "Epoch 1750, Generator Loss: 56.93260, Discriminator Loss: -3.25344\n",
      "Epoch 1760, Generator Loss: 56.93334, Discriminator Loss: -3.19177\n",
      "Epoch 1770, Generator Loss: 56.90687, Discriminator Loss: -3.17434\n",
      "Epoch 1780, Generator Loss: 56.93643, Discriminator Loss: -3.21489\n",
      "Epoch 1790, Generator Loss: 57.07465, Discriminator Loss: -3.21507\n",
      "Epoch 1800, Generator Loss: 56.90904, Discriminator Loss: -3.25348\n",
      "Epoch 1810, Generator Loss: 57.11715, Discriminator Loss: -2.99527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_gan(gen, dis, coarse_data_train, fine_data_train, clip_value, epochs, batch_size, save_intermediate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468dabb-f8c7-4821-9ff1-5e665193a832",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Unset the random seed to generate ensembles\n",
    "# tf.random.set_seed(None) \n",
    "\n",
    "# downscaled_data_mem1 = gen.predict( coarse_data_test.values.reshape( coarse_data_test.shape + (1,) ) )\n",
    "# downscaled_data_mem2 = gen.predict( coarse_data_test.values.reshape( coarse_data_test.shape + (1,) ) )\n",
    "# downscaled_data_mem3 = gen.predict( coarse_data_test.values.reshape( coarse_data_test.shape + (1,) ) )\n",
    "# downscaled_data_mem4 = gen.predict( coarse_data_test.values.reshape( coarse_data_test.shape + (1,) ) )\n",
    "# downscaled_data_mem5 = gen.predict( coarse_data_test.values.reshape( coarse_data_test.shape + (1,) ) )\n",
    "# downscaled_data_mem6 = gen.predict( coarse_data_test.values.reshape( coarse_data_test.shape + (1,) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74955d-4164-484f-a620-1e14c2940853",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# downscaled_data_mem1 = xr.DataArray(name='precipitation', data=downscaled_data_mem1.squeeze(), \n",
    "#                                dims=fine_data_test.dims, coords=fine_data_test.coords, \n",
    "#                                attrs=fine_data_test.attrs)\n",
    "# downscaled_data_mem1 = downscaled_data_mem1.where(~fine_nan_mask, np.nan)\n",
    "\n",
    "# downscaled_data_mem2 = xr.DataArray(name='precipitation', data=downscaled_data_mem2.squeeze(), \n",
    "#                                dims=fine_data_test.dims, coords=fine_data_test.coords, \n",
    "#                                attrs=fine_data_test.attrs)\n",
    "# downscaled_data_mem2 = downscaled_data_mem2.where(~fine_nan_mask, np.nan)\n",
    "\n",
    "# downscaled_data_mem3 = xr.DataArray(name='precipitation', data=downscaled_data_mem3.squeeze(), \n",
    "#                                dims=fine_data_test.dims, coords=fine_data_test.coords, \n",
    "#                                attrs=fine_data_test.attrs)\n",
    "# downscaled_data_mem3 = downscaled_data_mem3.where(~fine_nan_mask, np.nan)\n",
    "\n",
    "# downscaled_data_mem4 = xr.DataArray(name='precipitation', data=downscaled_data_mem4.squeeze(), \n",
    "#                                dims=fine_data_test.dims, coords=fine_data_test.coords, \n",
    "#                                attrs=fine_data_test.attrs)\n",
    "# downscaled_data_mem4 = downscaled_data_mem4.where(~fine_nan_mask, np.nan)\n",
    "\n",
    "# downscaled_data_mem5 = xr.DataArray(name='precipitation', data=downscaled_data_mem5.squeeze(), \n",
    "#                                dims=fine_data_test.dims, coords=fine_data_test.coords, \n",
    "#                                attrs=fine_data_test.attrs)\n",
    "# downscaled_data_mem5 = downscaled_data_mem5.where(~fine_nan_mask, np.nan)\n",
    "\n",
    "# downscaled_data_mem6 = xr.DataArray(name='precipitation', data=downscaled_data_mem6.squeeze(), \n",
    "#                                dims=fine_data_test.dims, coords=fine_data_test.coords, \n",
    "#                                attrs=fine_data_test.attrs)\n",
    "# downscaled_data_mem6 = downscaled_data_mem6.where(~fine_nan_mask, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542dfdb-8663-4441-b780-2671076c4a2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# downscaled_data_list = [downscaled_data_mem1, downscaled_data_mem2, downscaled_data_mem3, \n",
    "#                    downscaled_data_mem4, downscaled_data_mem5, downscaled_data_mem6\n",
    "#                   ]\n",
    "# downscaled_data = xr.concat(downscaled_data_list, dim='number')\n",
    "# downscaled_data = downscaled_data.assign_coords(number=np.arange( len(downscaled_data_list)) )\n",
    "# downscaled_data = downscaled_data.transpose('time', 'lat', 'lon', 'number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af789b1c-4f8d-4bfb-8636-f0019fc72d49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fine_data_test = fine_data_test.where(~fine_nan_mask, np.nan)\n",
    "# coarse_data_test = coarse_data_test.where(~coarse_nan_mask, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e60550-1412-4666-9aef-ef1405a582a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# diro = '/work/bb0983/athul_satheesh/downscaled_data/europe/downscaled_data/'\n",
    "# # downscaled_data.to_netcdf(diro + 'e_obs_eu_downscaled_wgan_prob.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c82756-fa6d-4a2b-8099-a0e6325604dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# gen.save('/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_generator_prob.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226c769-f4bd-4182-8669-8ff056ed04ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# gen.save('/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_generator_prob.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee2cfa-4b4a-47bd-bf3d-e24a0afc79a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# date1 = '2021-06-15'\n",
    "# date2 = '2021-09-15'\n",
    "\n",
    "# # print(f\"CRPS: {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data) ):.4f}\")\n",
    "# print(f\"MAE1: {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data.sel(number=0)) ):.4f}\")\n",
    "# print(f\"MAE2: {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data.sel(number=1)) ):.4f}\")\n",
    "# print(f\"MAE3: {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data.sel(number=2)) ):.4f}\")\n",
    "# print(f\"MAE4: {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data.sel(number=3)) ):.4f}\")\n",
    "# print(f\"MAE5: {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data.sel(number=4)) ):.4f}\")\n",
    "# print(f\"MAE6: {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data.sel(number=5)) ):.4f}\")\n",
    "# print(f\"MAE : {np.nanmean(ps.crps_ensemble(observations=fine_data_test, forecasts=downscaled_data.mean(dim='number')) ):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ebaa1-afa7-4fb9-a881-9eacf358b77f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.corrcoef(fine_data_test.values.flatten(), downscaled_data.values.flatten())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f6e4e-882f-49a0-bbaf-178fe9535f40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "# def compute_r2_with_nans(y_true, y_pred):\n",
    "#     # Mask for non-NaN values\n",
    "#     mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    \n",
    "#     # Filter out NaN values\n",
    "#     y_true_filtered = y_true[mask]\n",
    "#     y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "#     # Compute R2 score\n",
    "#     r2 = r2_score(y_true_filtered, y_pred_filtered)\n",
    "#     return r2\n",
    "# r2 = compute_r2_with_nans(fine_data_test.values.flatten(), downscaled_data.mean(dim='number').values.flatten())\n",
    "# r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf31a66-4b47-4aeb-b16c-771113de4f79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figs, axx = plt.subplots(figsize=(8,6))\n",
    "# axx.scatter(fine_data_test.values.flatten(), downscaled_data.values.flatten(), s=0.1, alpha=0.9, color='C0')\n",
    "# axx.plot([0,1000], [0,1000], color='red', alpha=0.9, zorder=1, ls='--')\n",
    "# axx.set_xlim(-8,500)\n",
    "# axx.set_ylim(-8,500)\n",
    "# axx.set_xlabel('Observation', size=15)\n",
    "# axx.set_ylabel('Downscaled', size=15)\n",
    "# axx.text(45,400,f'$R^2: {r2:.2f}$', color='white', size=12)\n",
    "# axx.grid(True, alpha=0.2, color='C1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5790a-3a04-4728-bf1f-c44a3e63b68e",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# crps_data = ps.crps_ensemble(observations=fine_data_test.sel(time=slice(date1, date2)), \n",
    "#                              forecasts=downscaled_data.sel(time=slice(date1, date2)))\n",
    "# # Convert to xarray DataArray if necessary\n",
    "# crps_data = xr.DataArray(crps_data, dims=[\"time\", \"lat\", \"lon\"], coords={\"time\": fine_data_test.sel(time=slice(date1, date2)).time,\n",
    "#                                                                          \"lat\": fine_data_test.lat, \n",
    "#                                                                          \"lon\": fine_data_test.lon,\n",
    "#                                                                         })\n",
    "\n",
    "# # Define the colormap colors\n",
    "# colors_blues2black = [(0, 0, 0), (0, 0.90, 1)]  # Blue to Black\n",
    "# colors_RdBlBu = [(1, 0, 0), (0, 0, 0), (0, 0.90, 1)]  # Blue to Black\n",
    "\n",
    "# # Create the colormap\n",
    "# cmap_name = 'BluesToBlack'\n",
    "# blues_to_black = mcolors.LinearSegmentedColormap.from_list(cmap_name, colors_blues2black)\n",
    "# Rd_bl_Bu = mcolors.LinearSegmentedColormap.from_list('RdBlBu', colors_RdBlBu)\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12,10), sharex=True, sharey=True,\n",
    "#                         subplot_kw=dict(projection=ccrs.PlateCarree(), facecolor='black'),\n",
    "#                         gridspec_kw={'wspace': -0.05, 'hspace': -0.45})\n",
    "\n",
    "# # Define the plots list\n",
    "# plots = [coarse_data_test, fine_data_test, downscaled_data.mean(dim='number'), crps_data]\n",
    "# cmap = [blues_to_black, blues_to_black, blues_to_black, 'afmhot']\n",
    "# levels = [np.arange(0,6.125,0.125), np.arange(0,6.125,0.125), np.arange(0,6.125,0.125), np.arange(0,6.125,0.125)]\n",
    "# title = ['a) Coarse','b) Fine','c) Downscaled','d) CRPS']\n",
    "# ticks = [np.arange(0,7,1), np.arange(0,7,1), np.arange(0,7,1), np.arange(0,7,1)]\n",
    "\n",
    "# for i, ax in enumerate(axs.flatten()):\n",
    "#     if i != (len(plots) - 1):\n",
    "#         plots[i].sel(time=slice(date1, date2)).mean('time').plot(cmap=cmap[i], levels=levels[i], \n",
    "#                                                                  ax=ax, cbar_kwargs={'orientation':'vertical',\n",
    "#                                                                                     'pad':0.01, 'label':'',\n",
    "#                                                                                     'shrink':0.48, 'drawedges':False,\n",
    "#                                                                                     'ticks': ticks[i], },\n",
    "#                                                                  alpha=0.90)\n",
    "#     else:\n",
    "#         plots[-1].sel(time=slice(date1, date2)).mean('time').plot(cmap=cmap[i], levels=levels[i], \n",
    "#                        ax=ax, cbar_kwargs={'orientation':'vertical',\n",
    "#                                            'pad':0.01, 'label':'',\n",
    "#                                            'shrink':0.48, 'drawedges':False,\n",
    "#                                            'ticks': ticks[i], },\n",
    "#                        alpha=0.90)\n",
    "    \n",
    "#     ax.text(0, 55.2, f'{title[i]}', size=13, color='white')\n",
    "#     ax.coastlines(linewidth=1.5, color='white')\n",
    "#     ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=1.5, edgecolor='white')\n",
    "#     ax.patch.set_facecolor('black')\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
