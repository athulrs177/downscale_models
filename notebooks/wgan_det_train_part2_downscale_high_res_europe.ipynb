{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69462c4e-76ba-47a6-a78f-4ff8b63e5aea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 13:17:40.624109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 13:17:40.624150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 13:17:40.625311: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 13:17:40.632165: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 13:17:41.815654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Conv2DTranspose, MaxPool2D, Flatten, Conv2D, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization, UpSampling2D, concatenate, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Input\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Dropout, Concatenate, LeakyReLU, BatchNormalization, Add\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm \n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MaxAbsScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "tf.random.set_seed(42)  # to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a063a2-6f1c-4924-8242-425a88cfb819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137f9c7f-5d41-4a53-9db0-a4a9aa56a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 13:17:45.107832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 75826 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2024-08-21 13:17:45.109348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 75826 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:44:00.0, compute capability: 8.0\n",
      "2024-08-21 13:17:45.110751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 75826 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:84:00.0, compute capability: 8.0\n",
      "2024-08-21 13:17:45.112145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 75826 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:c4:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Get the list of available physical GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Set memory growth for each GPU\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d876839-26e3-482c-a342-5824d9f0c4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diri = '/work/bb0983/athul_satheesh/e_obs_precip/'\n",
    "diro = '/work/bb0983/athul_satheesh/downscaled_data/europe/figures/'\n",
    "\n",
    "coarse_raw = 'rr_ens_mean_1.0deg_reg_v29.0e.nc'\n",
    "fine_raw = 'rr_ens_mean_0.1deg_reg_v29.0e.nc'\n",
    "\n",
    "lati = 43#40\n",
    "latf = 59#60\n",
    "\n",
    "loni = -6#-10\n",
    "lonf = 15#30\n",
    "\n",
    "strt = '1950-01-01'\n",
    "last = '2023-12-31'\n",
    "\n",
    "coarse_data = xr.open_dataset(diri+coarse_raw).rr.transpose('time','lat','lon').sel(time=slice(strt, last), \n",
    "                                                                                    lat=slice(lati, latf), \n",
    "                                                                                    lon=slice(loni, lonf)\n",
    "                                                                                   )\n",
    "fine_data = xr.open_dataset(diri+fine_raw).rr.transpose('time','latitude','longitude').sel(time=slice(strt, last), \n",
    "                                                                                           latitude=slice(lati, latf), \n",
    "                                                                                           longitude=slice(loni, lonf)\n",
    "                                                                                          )\n",
    "fine_data = fine_data.rename({'latitude':'lat', 'longitude':'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5df139-59b3-4c0f-b01f-db6c717413f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_coarse = coarse_data.shape\n",
    "dims_fine = fine_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62136626-8e1a-4880-9706-1b7fbac4de4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27028, 16, 21), (27028, 160, 210))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims_coarse, dims_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e96e34b-64dd-40a1-a62c-464150bfd655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_strt = strt\n",
    "train_last = '2000-12-31'\n",
    "\n",
    "test_strt = '2001-01-01'\n",
    "test_last = last\n",
    "\n",
    "coarse_data_train = coarse_data.sel(time=slice(train_strt, train_last))#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_train = coarse_data_train.fillna(coarse_data_train.mean())\n",
    "coarse_data_test = coarse_data.sel(time=slice(test_strt, test_last))#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_test = coarse_data_test.fillna(coarse_data_test.mean())\n",
    "# print(coarse_data_train.shape, coarse_data_test.shape)\n",
    "\n",
    "fine_data_train = fine_data.sel(time=slice(train_strt, train_last))\n",
    "fine_data_test = fine_data.sel(time=slice(test_strt, test_last))\n",
    "# print(fine_data_train.shape, fine_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c02225-58ae-43f0-99e2-10aebc02f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_nan_mask = np.isnan(coarse_data_test)\n",
    "fine_nan_mask = np.isnan(fine_data_test)\n",
    "\n",
    "fill_val = -1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b631b060-a6bb-4fe6-90ac-55b42c993ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_data_train = coarse_data_train.fillna(fill_val)#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_train = coarse_data_train.fillna(coarse_data_train.mean())\n",
    "coarse_data_test = coarse_data_test.fillna(fill_val)#.interp(lat=fine_data.lat, lon=fine_data.lon, method='linear')\n",
    "# coarse_data_test = coarse_data_test.fillna(coarse_data_test.mean())\n",
    "# print(coarse_data_train.shape, coarse_data_test.shape)\n",
    "\n",
    "fine_data_train = fine_data_train.fillna(fill_val)\n",
    "fine_data_test = fine_data_test.fillna(fill_val)\n",
    "# print(fine_data_train.shape, fine_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa2aa60c-3df9-42ae-8a88-c36d736c1e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 160, 210)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df65693-9401-4fb3-b807-624a93043b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_loss(fake_images, real_images):\n",
    "    \"\"\"\n",
    "    Computes the SSIM loss between fake and real images.\n",
    "    \n",
    "    Parameters:\n",
    "        fake_images (tf.Tensor): Generated images.\n",
    "        real_images (tf.Tensor): Real images.\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: SSIM loss.\n",
    "    \"\"\"\n",
    "    ssim_index = tf.image.ssim(fake_images, real_images, max_val=1.0)\n",
    "    return 1 - tf.reduce_mean(ssim_index)\n",
    "\n",
    "def generator_loss(fake_output, fake_images, real_images, penalty_weight=15, ssim_weight=15):\n",
    "    \"\"\"\n",
    "    Generator loss function using Wasserstein loss with an added penalty term and SSIM loss.\n",
    "    \n",
    "    Parameters:\n",
    "        fake_output (tf.Tensor): Output of the discriminator when given generated images.\n",
    "        fake_images (tf.Tensor): Generated images.\n",
    "        real_images (tf.Tensor): Real images.\n",
    "        penalty_weight (float): Weight of the penalty term.\n",
    "        ssim_weight (float): Weight of the SSIM loss term.\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: Generator loss.\n",
    "    \"\"\"\n",
    "    wasserstein_loss = -tf.reduce_mean(fake_output)\n",
    "    \n",
    "    # Penalty term for deviation from real outputs\n",
    "    # penalty = penalty_weight * tf.reduce_mean(tf.abs(fake_output - real_output))\n",
    "    penalty = penalty_weight * tf.reduce_mean(tf.abs(fake_images - real_images))\n",
    "    \n",
    "    # SSIM loss\n",
    "    ssim_loss_value = ssim_loss(fake_images, real_images)\n",
    "    \n",
    "    return wasserstein_loss + penalty + ssim_weight * ssim_loss_value\n",
    "    # return wasserstein_loss + ssim_weight * ssim_loss_value\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    Discriminator loss function using Wasserstein loss.\n",
    "    \n",
    "    Parameters:\n",
    "        real_output (tf.Tensor): Output of the discriminator when given real images.\n",
    "        fake_output (tf.Tensor): Output of the discriminator when given generated images.\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: Discriminator loss.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dfff4f-9339-4189-93bc-156b79d5514d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weight clipping\n",
    "def clip_weights(model, clip_value):\n",
    "    \"\"\"\n",
    "    Clips the weights of the model to be within the range [-clip_value, clip_value].\n",
    "    \n",
    "    Parameters:\n",
    "        model (tf.keras.Model): The model whose weights will be clipped.\n",
    "        clip_value (float): The value to clip the weights to.\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            kernel = layer.kernel\n",
    "            clipped_kernel = tf.clip_by_value(kernel, -clip_value, clip_value)\n",
    "            layer.kernel.assign(clipped_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd66db0-32df-4ee3-82a4-2209d9ae72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_in = 100\n",
    "\n",
    "model_diri = '/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/' #'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp/'\n",
    "gen_model_fili = f'generator_det_epoch_{epoch_in}_adamV2.keras'\n",
    "dis_model_fili = f'discriminator_det_epoch_{epoch_in}_adamV2.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b4a52d5-1aef-4f4f-b49a-e04f42afe606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "input_shape  = dims_coarse[1:] + (1,)\n",
    "output_shape = dims_fine[1:] + (1,)\n",
    "\n",
    "# lr = 0.5e-5\n",
    "gen_lr = 1e-4 # updated to 1e-4 from 2e-4 100 epochs\n",
    "dis_lr = 0.5e-5 # updated to 0.5e-5 from 2e-4 after 100 epochs\n",
    "clip_value = 1e-3\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    # gen_optimizer = RMSprop(learning_rate=lr, momentum=0.5)\n",
    "    # dis_optimizer = RMSprop(learning_rate=lr, momentum=0.5)\n",
    "    gen_optimizer = Adam(learning_rate=gen_lr)\n",
    "    dis_optimizer = Adam(learning_rate=dis_lr)\n",
    "\n",
    "    # Load the generator and discriminator models\n",
    "    gen = load_model(model_diri+gen_model_fili)\n",
    "    dis = load_model(model_diri+dis_model_fili)    \n",
    "\n",
    "    gen.compile(optimizer=gen_optimizer,)\n",
    "    dis.compile(optimizer=dis_optimizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4838ac4-2d1f-46a0-9bf3-acc9bcee8ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " generator_input (InputLaye  [(None, 16, 21, 1)]       0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " conv_4_conv (Conv2D)        (None, 16, 21, 16)        160       \n",
      "                                                                 \n",
      " conv_4_lrelu (LeakyReLU)    (None, 16, 21, 16)        0         \n",
      "                                                                 \n",
      " conv_4_bn (BatchNormalizat  (None, 16, 21, 16)        64        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_3_conv (Conv2D)        (None, 16, 21, 32)        4640      \n",
      "                                                                 \n",
      " conv_3_lrelu (LeakyReLU)    (None, 16, 21, 32)        0         \n",
      "                                                                 \n",
      " conv_3_bn (BatchNormalizat  (None, 16, 21, 32)        128       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_2_conv (Conv2D)        (None, 16, 21, 64)        18496     \n",
      "                                                                 \n",
      " conv_2_lrelu (LeakyReLU)    (None, 16, 21, 64)        0         \n",
      "                                                                 \n",
      " conv_2_bn (BatchNormalizat  (None, 16, 21, 64)        256       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_1_conv (Conv2D)        (None, 16, 21, 128)       73856     \n",
      "                                                                 \n",
      " conv_1_lrelu (LeakyReLU)    (None, 16, 21, 128)       0         \n",
      "                                                                 \n",
      " conv_1_bn (BatchNormalizat  (None, 16, 21, 128)       512       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_0_conv (Conv2D)        (None, 16, 21, 256)       295168    \n",
      "                                                                 \n",
      " conv_0_lrelu (LeakyReLU)    (None, 16, 21, 256)       0         \n",
      "                                                                 \n",
      " conv_0_bn (BatchNormalizat  (None, 16, 21, 256)       1024      \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_00_conv (Conv2D)       (None, 16, 21, 512)       1180160   \n",
      "                                                                 \n",
      " conv_00_lrelu (LeakyReLU)   (None, 16, 21, 512)       0         \n",
      "                                                                 \n",
      " conv_00_bn (BatchNormaliza  (None, 16, 21, 512)       2048      \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " deconv_00_deconv (Conv2DTr  (None, 16, 21, 512)       2359808   \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " deconv_00_lrelu (LeakyReLU  (None, 16, 21, 512)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " deconv_00_bn (BatchNormali  (None, 16, 21, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " deconv_0_deconv (Conv2DTra  (None, 16, 21, 256)       1179904   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_0_lrelu (LeakyReLU)  (None, 16, 21, 256)       0         \n",
      "                                                                 \n",
      " deconv_0_bn (BatchNormaliz  (None, 16, 21, 256)       1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_1_deconv (Conv2DTra  (None, 16, 21, 128)       295040    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_1_lrelu (LeakyReLU)  (None, 16, 21, 128)       0         \n",
      "                                                                 \n",
      " deconv_1_bn (BatchNormaliz  (None, 16, 21, 128)       512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_2_deconv (Conv2DTra  (None, 16, 21, 64)        73792     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_2_lrelu (LeakyReLU)  (None, 16, 21, 64)        0         \n",
      "                                                                 \n",
      " deconv_2_bn (BatchNormaliz  (None, 16, 21, 64)        256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_3_deconv (Conv2DTra  (None, 16, 21, 32)        18464     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_3_lrelu (LeakyReLU)  (None, 16, 21, 32)        0         \n",
      "                                                                 \n",
      " deconv_3_bn (BatchNormaliz  (None, 16, 21, 32)        128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " deconv_4_deconv (Conv2DTra  (None, 80, 105, 16)       18448     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " deconv_4_lrelu (LeakyReLU)  (None, 80, 105, 16)       0         \n",
      "                                                                 \n",
      " deconv_4_bn (BatchNormaliz  (None, 80, 105, 16)       64        \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " final_deconv (Conv2DTransp  (None, 160, 210, 1)       145       \n",
      " ose)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5526145 (21.08 MB)\n",
      "Trainable params: 5522113 (21.07 MB)\n",
      "Non-trainable params: 4032 (15.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a532b24-5242-48d5-b62c-df4de0b79378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"wGAN_discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " discriminator_input (Input  [(None, 160, 210, 1)]     0         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " conv2_conv (Conv2D)         (None, 80, 105, 128)      1280      \n",
      "                                                                 \n",
      " conv2_lrelu (LeakyReLU)     (None, 80, 105, 128)      0         \n",
      "                                                                 \n",
      " conv2_bn (BatchNormalizati  (None, 80, 105, 128)      512       \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv4_conv (Conv2D)         (None, 80, 105, 256)      295168    \n",
      "                                                                 \n",
      " conv4_lrelu (LeakyReLU)     (None, 80, 105, 256)      0         \n",
      "                                                                 \n",
      " conv4_bn (BatchNormalizati  (None, 80, 105, 256)      1024      \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv6_conv (Conv2D)         (None, 80, 105, 512)      1180160   \n",
      "                                                                 \n",
      " conv6_lrelu (LeakyReLU)     (None, 80, 105, 512)      0         \n",
      "                                                                 \n",
      " conv6_bn (BatchNormalizati  (None, 80, 105, 512)      2048      \n",
      " on)                                                             \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 40, 52, 512)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40, 52, 512)       0         \n",
      "                                                                 \n",
      " final_conv (Conv2D)         (None, 40, 52, 1)         4609      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1484801 (5.66 MB)\n",
      "Trainable params: 1483009 (5.66 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "160299f8-d49d-4cc5-90fa-c48fc83fc019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "def preprocess_data(data):\n",
    "    data = tf.convert_to_tensor(data.values, dtype=tf.float32)\n",
    "    return tf.reshape(data, data.shape + (1,))\n",
    "# Compute and update gradients\n",
    "def train_step(coarse_data_batch, fine_data_batch, dis, clip_value):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "        fake_data_batch = gen(coarse_data_batch, training=True)\n",
    "        real_output = dis(fine_data_batch, training=True)\n",
    "        fake_output = dis(fake_data_batch, training=True)\n",
    "        gen_loss = generator_loss(fake_output, fake_data_batch, fine_data_batch)\n",
    "        dis_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_gen = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
    "    gradients_of_dis = dis_tape.gradient(dis_loss, dis.trainable_variables)\n",
    "    \n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_gen, gen.trainable_variables))\n",
    "    dis_optimizer.apply_gradients(zip(gradients_of_dis, dis.trainable_variables))\n",
    "    \n",
    "    # Clip discriminator weights\n",
    "    clip_weights(dis, clip_value)\n",
    "    \n",
    "    return gen_loss, dis_loss\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(coarse_data_batch, fine_data_batch, dis, clip_value):\n",
    "    per_replica_gen_losses, per_replica_dis_losses = strategy.run(train_step, args=(coarse_data_batch, fine_data_batch, dis, clip_value))\n",
    "    mean_gen_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_gen_losses, axis=None)\n",
    "    mean_dis_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_dis_losses, axis=None)\n",
    "    return mean_gen_loss, mean_dis_loss\n",
    "\n",
    "def train_gan(gen, dis, coarse_data_train, fine_data_train, clip_value, epochs, batch_size, save_intermediate=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((preprocess_data(coarse_data_train), preprocess_data(fine_data_train)))\n",
    "    # dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat(epochs)\n",
    "    dataset = dataset.shuffle(buffer_size=365*2).batch(batch_size)#.repeat(epochs)\n",
    "    distributed_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_gen_loss = 0.0\n",
    "        total_dis_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for coarse_data_batch, fine_data_batch in distributed_dataset:\n",
    "            gen_loss, dis_loss = distributed_train_step(coarse_data_batch, fine_data_batch, dis, clip_value)\n",
    "            total_gen_loss += gen_loss\n",
    "            total_dis_loss += dis_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_gen_loss = total_gen_loss / num_batches\n",
    "        avg_dis_loss = total_dis_loss / num_batches\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Generator Loss: {avg_gen_loss:.5f}, Discriminator Loss: {avg_dis_loss:.5f}\")\n",
    "\n",
    "        # Save models every 100 epochs\n",
    "        if save_intermediate:\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                gen.save(f'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/generator_det_epoch_{epoch_in+epoch+1}_adamV2.keras')\n",
    "                dis.save(f'/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_extended_eu/discriminator_det_epoch_{epoch_in+epoch+1}_adamV2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0536200-3174-49d6-9da3-ccbc65b1000a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def normalize_data(dataset, scaler, train=True):\n",
    "    \n",
    "#     dims = dataset.shape\n",
    "    \n",
    "#     dataset = dataset.values.reshape(-1, dims[1]*dims[2])\n",
    "\n",
    "#     if train:\n",
    "#         normalized_data = scaler.fit_transform(dataset)\n",
    "#     else:\n",
    "#         normalized_data = scaler.transform(dataset)\n",
    "        \n",
    "#     normalized_data = normalized_data.reshape(dims + (1,) ) # Make 4D\n",
    "    \n",
    "#     return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd48624-396c-4587-af41-bbd009eb9d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 450 \n",
    "batch_size = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "015e29b3-4acf-4a3b-87c2-9024a8313cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 13:18:38.283251: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 50 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 14 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 50 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 14 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 13:18:53.966073: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inreplica_1/wGAN_discriminator/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-08-21 13:18:55.570090: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-21 13:18:55.603677: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-21 13:18:55.644402: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-21 13:18:55.658132: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-08-21 13:18:55.686576: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-08-21 13:21:14.066352: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd882d07eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-21 13:21:14.066392: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-21 13:21:14.066398: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-21 13:21:14.066402: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-21 13:21:14.066405: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-08-21 13:21:14.078724: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724239274.244232 2796017 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 50 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 14 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 13:21:42.851053: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inreplica_1/wGAN_discriminator/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Generator Loss: 36.92801, Discriminator Loss: -1.41284\n",
      "Epoch 20, Generator Loss: 37.16657, Discriminator Loss: -1.50220\n",
      "Epoch 30, Generator Loss: 37.32770, Discriminator Loss: -1.51201\n",
      "Epoch 40, Generator Loss: 36.43479, Discriminator Loss: -1.52489\n",
      "Epoch 50, Generator Loss: 35.96231, Discriminator Loss: -1.45975\n",
      "Epoch 60, Generator Loss: 34.86019, Discriminator Loss: -1.50837\n",
      "Epoch 70, Generator Loss: 34.89820, Discriminator Loss: -1.53198\n",
      "Epoch 80, Generator Loss: 34.38742, Discriminator Loss: -1.46730\n",
      "Epoch 90, Generator Loss: 34.30152, Discriminator Loss: -1.58649\n",
      "Epoch 100, Generator Loss: 33.96468, Discriminator Loss: -1.58204\n",
      "Epoch 110, Generator Loss: 34.95599, Discriminator Loss: -1.59402\n",
      "Epoch 120, Generator Loss: 33.75338, Discriminator Loss: -1.56530\n",
      "Epoch 130, Generator Loss: 33.36537, Discriminator Loss: -1.64474\n",
      "Epoch 140, Generator Loss: 34.01175, Discriminator Loss: -1.58774\n",
      "Epoch 150, Generator Loss: 33.15188, Discriminator Loss: -1.53760\n",
      "Epoch 160, Generator Loss: 32.41005, Discriminator Loss: -1.52483\n",
      "Epoch 170, Generator Loss: 32.60626, Discriminator Loss: -1.56703\n",
      "Epoch 180, Generator Loss: 32.35639, Discriminator Loss: -1.49681\n",
      "Epoch 190, Generator Loss: 32.37740, Discriminator Loss: -1.54293\n",
      "Epoch 200, Generator Loss: 31.68644, Discriminator Loss: -1.57067\n",
      "Epoch 210, Generator Loss: 31.08291, Discriminator Loss: -1.41297\n",
      "Epoch 220, Generator Loss: 30.96225, Discriminator Loss: -1.50850\n",
      "Epoch 230, Generator Loss: 32.29798, Discriminator Loss: -1.56584\n",
      "Epoch 240, Generator Loss: 30.80731, Discriminator Loss: -1.50436\n",
      "Epoch 250, Generator Loss: 30.88014, Discriminator Loss: -1.55627\n",
      "Epoch 260, Generator Loss: 31.04125, Discriminator Loss: -1.54805\n",
      "Epoch 270, Generator Loss: 30.71524, Discriminator Loss: -1.42808\n",
      "Epoch 280, Generator Loss: 30.28964, Discriminator Loss: -1.54144\n",
      "Epoch 290, Generator Loss: 30.18646, Discriminator Loss: -1.52142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_gan(gen, dis, coarse_data_train, fine_data_train, clip_value, epochs, batch_size, save_intermediate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a85eee6-8a66-479c-9203-07aa6c449b68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def inverse_normalize_data(normalized_data, scaler):\n",
    "    \n",
    "#     dims = normalized_data.shape\n",
    "    \n",
    "#     normalized_data = normalized_data.reshape(-1, dims[1]*dims[2])\n",
    "    \n",
    "#     original_data = scaler.inverse_transform(normalized_data)\n",
    "    \n",
    "#     original_data = original_data.reshape(dims)\n",
    "#     return original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4468dabb-f8c7-4821-9ff1-5e665193a832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AtomicFunction.__del__ at 0x7ffb412f89d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382999/.conda/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 286, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "downscaled_data = gen.predict( coarse_data_test.values.reshape( coarse_data_test.shape + (1,) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0cf8a-a50d-44e7-b636-0e8536cd67b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downscaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fd189-3b3f-4ba1-89f0-ee4fa0fc4e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# downscaled_data = inverse_normalize_data(downscaled_data, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f0bed-cbf5-4571-b4ca-37537adab2fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downscaled_data = xr.DataArray(name='precipitation', data=downscaled_data.squeeze(), \n",
    "                               dims=fine_data_test.dims, coords=fine_data_test.coords, \n",
    "                               attrs=fine_data_test.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af789b1c-4f8d-4bfb-8636-f0019fc72d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled_data = downscaled_data.where(~fine_nan_mask, np.nan)\n",
    "fine_data_test = fine_data_test.where(~fine_nan_mask, np.nan)\n",
    "coarse_data_test = coarse_data_test.where(~coarse_nan_mask, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e60550-1412-4666-9aef-ef1405a582a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diro = '/work/bb0983/athul_satheesh/downscaled_data/europe/downscaled_data/'\n",
    "# downscaled_data.to_netcdf(diro + 'e_obs_eu_downscaled_wgan_transpose.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c82756-fa6d-4a2b-8099-a0e6325604dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# gen.save('/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_generator_transpose.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226c769-f4bd-4182-8669-8ff056ed04ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# dis.save('/work/bb0983/athul_satheesh/downscaled_data/europe/trained_models/wgan_gp_discriminator_transpose.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee2cfa-4b4a-47bd-bf3d-e24a0afc79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = '2021-06-15'\n",
    "date2 = '2021-09-15'\n",
    "\n",
    "print(f\"MAE: {abs(fine_data_test - downscaled_data).mean().values:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt( ( (fine_data_test - downscaled_data)**2 ).mean() ).values:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ebaa1-afa7-4fb9-a881-9eacf358b77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.corrcoef(fine_data_test.values.flatten(), downscaled_data.values.flatten())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f6e4e-882f-49a0-bbaf-178fe9535f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def compute_r2_with_nans(y_true, y_pred):\n",
    "    # Mask for non-NaN values\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    # Compute R2 score\n",
    "    r2 = r2_score(y_true_filtered, y_pred_filtered)\n",
    "    return r2\n",
    "r2 = compute_r2_with_nans(fine_data_test.values.flatten(), downscaled_data.values.flatten())\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf31a66-4b47-4aeb-b16c-771113de4f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figs, axx = plt.subplots(figsize=(8,6))\n",
    "# axx.scatter(fine_data_test.values.flatten(), downscaled_data.values.flatten(), s=0.1, alpha=0.9, color='C0')\n",
    "# axx.plot([0,1000], [0,1000], color='red', alpha=0.9, zorder=1, ls='--')\n",
    "# axx.set_xlim(-8,500)\n",
    "# axx.set_ylim(-8,500)\n",
    "# axx.set_xlabel('Observation', size=15)\n",
    "# axx.set_ylabel('Downscaled', size=15)\n",
    "# axx.text(45,400,f'$R^2: {r2:.2f}$', color='white', size=12)\n",
    "# axx.grid(True, alpha=0.2, color='C1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5790a-3a04-4728-bf1f-c44a3e63b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colormap colors\n",
    "colors_blues2black = [(0, 0, 0), (0, 0.90, 1)]  # Blue to Black\n",
    "colors_RdBlBu = [(1, 0, 0), (0, 0, 0), (0, 0.90, 1)]  # Blue to Black\n",
    "\n",
    "# Create the colormap\n",
    "cmap_name = 'BluesToBlack'\n",
    "blues_to_black = mcolors.LinearSegmentedColormap.from_list(cmap_name, colors_blues2black)\n",
    "Rd_bl_Bu = mcolors.LinearSegmentedColormap.from_list('RdBlBu', colors_RdBlBu)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12,10), sharex=True, sharey=True,\n",
    "                        subplot_kw=dict(projection=ccrs.PlateCarree(), facecolor='black'),\n",
    "                        gridspec_kw={'wspace': -0.05, 'hspace': -0.45})\n",
    "\n",
    "# plots = [coarse_data_test, fine_data_test, downscaled_data, (fine_data_test - downscaled_data)]\n",
    "plots = [coarse_data_test, fine_data_test, downscaled_data, abs(fine_data_test - downscaled_data)]\n",
    "cmap = [blues_to_black, blues_to_black, blues_to_black, 'afmhot']\n",
    "# cmap = [blues_to_black, blues_to_black, blues_to_black, Rd_bl_Bu]\n",
    "# levels = [np.arange(0,30.25,0.25), np.arange(0,30.25,0.25), np.arange(0,30.25,0.25), np.arange(-12,12.25,0.25)]\n",
    "levels = [np.arange(0,6.125,0.125), np.arange(0,6.125,0.125), np.arange(0,6.125,0.125), np.arange(0,6.125,0.125)]\n",
    "# title = ['a) Coarse','b) Fine','c) Downscaled','d) Observed-Downscaled']\n",
    "title = ['a) Coarse','b) Fine','c) Downscaled','d) MAE']\n",
    "# ticks = [np.arange(0,33,3), np.arange(0,33,3), np.arange(0,33,3), np.arange(-12,13,4)]\n",
    "ticks = [np.arange(0,7,1), np.arange(0,7,1), np.arange(0,7,1), np.arange(0,7,1)]\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    plots[i].sel(time=slice(date1, date2)).mean('time').plot(cmap=cmap[i], levels=levels[i], \n",
    "                                                             ax=ax, cbar_kwargs={'orientation':'vertical',\n",
    "                                                                                'pad':0.01, 'label':'',\n",
    "                                                                                'shrink':0.48, 'drawedges':False,\n",
    "                                                                                'ticks': ticks[i], },\n",
    "                                                            alpha=0.90)\n",
    "    \n",
    "    ax.text(0, 55.2, f'{title[i]}',size=13, color='white')\n",
    "    ax.coastlines(linewidth=1.5, color='white')\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=1.5, edgecolor='white');\n",
    "    ax.patch.set_facecolor('black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab93da3-55be-4f67-af42-ecc02a4cd437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
